# Runtime settings
dry_run: true
use_composer: false

# Data configuration
training_data_path: null  # Required: Set in specific dataset config
validation_data_path: null   # Required: Set in specific dataset config
test_data_path: null        # Required: Set in specific dataset config

# Training process settings
log_interval: 1
val_interval: 5000
self_consistency_ratio: 0.25
max_steps: 50000
gradient_clipping: 2.0

# Optimizer configuration
optimizer:
  scheduler:
    type: "myle"
    lr: 3e-4
    weight_decay: 0.1
    warmup_steps: 100
    start_lr: 1e-7

# Example of linear scheduler configuration (commented out)
# optimizer:
#   scheduler:
#     type: "linear"
#     lr: 1e-4
#     weight_decay: 0.1
#     start_factor: 0.3
#     end_factor: 0.1
#     total_steps: 10000

batch_size: 256
accumulate_grad_batches: 8

deterministic: true
seed: 44


# Model configuration
model_config:
  input_dims: 768
  hidden_size: 768
  output_dims: 768
  hidden_t_dim: 128
  diffusion_steps: 2048
  min_shortcut_size: 32
  dropout: 0.1
  config_name: "bert-base-uncased"
  vocab_size: 30522
  init_pretrained: "no"
  logits_mode: 1
  sc_rate: 0.5
  predict_t: false
  max_position_embeddings: null

# WandB configuration
wandb:
  project_name: "test"
  run_name: "SFM_v0"
  resume: "allow"
  enabled: true

# Checkpoint configuration
checkpoint:
  save_folder: "checkpoints/test"
#  save_interval: 5000
  num_to_keep: -1
  overwrite: false
  save_last: true
  save_top_k: -1
  monitor: null
  mode: "min"

# EMA configuration
ema:
  smoothing: 0.99
  half_life: null
  update_interval: 1

