# QQP Training Configuration
# Extends default.yaml with QQP-specific settings

dry_run: false
use_exca: false

infra:
  cluster: "slurm"
  folder: "jobs"
  timeout_min: 10080
  mode: "force"
  slurm_partition: "hgx"
  gpus_per_node: 2
  tasks_per_node: 2
  cpus_per_task: 8
  slurm_use_srun: true
  slurm_additional_parameters:
    nodelist: "hgx1"


# Data paths
training_data_path: "datasets/tokenized/bert-base-uncased/QQP-Official/train"
validation_data_path: "datasets/tokenized/bert-base-uncased/QQP-Official/valid"
padding_strategy:
  mark_first_padding: true
  mark_second_padding: true

check_val_every_n_epoch: 10000
val_interval: null

denoising_step_size: 512  # Step size for denoising process
num_val_batches_to_log: 1  # Number of validation batches to log predictions for
num_timestep_bins: 4
prediction_shortcut_size: 512
log_train_predictions_every_n_epochs: 200  # Number of epochs between train prediction logging
log_train_predictions_from_n_epochs: 200  # Number of epochs between train prediction logging

limit_train_batches: null
limit_val_batches: 0
overfit_batches: 1

max_steps: 5000
gradient_clipping: null
reduce_fn: "mean"

batch_size: 8
accumulate_grad_batches: 1

architecture: "transformer"

seed: 102

#loss:
#  type: vmf
#  mvf_loss_config:
#    regularization_type: cosine_penalized

normalize_flow_matching_loss: false

flow_matching_loss_weight: 1.0
consistency_loss_weight: 1.0
nll_loss_weight: 1.0
isotropy_loss_weight: 0.0

consistency_start_step: 10

# Classifier-free guidance settings
cfg_guidance_scale: 1.0  # Set to 1.0 for no guidance, >1.0 to enable guidance (e.g., 3.0)
cfg_start_step: null  # Global step to start applying classifier-free guidance, null for disabling cfg
cfg_probability: 0.5  # Probability of training unconditionally (discarding conditioning)

self_consistency_ratio: 0.25
model:
  init_pretrained: bert
  config_name: bert-base-uncased
  use_pretrained_weights: false
  freeze_word_embedding: false
  vocab_size: 30522
  word_embedding_std: 1.0
  input_dims: 128
  output_dims: 128
  sc_rate: 0.0
  parametrization: "x0"
  stacked_embeddings: false
  hidden_t_dim: 128
  hidden_shortcut_dim: 128
  projection_activation: "tanh"
  normalize_word_embedding: false
  scale_time: false

time_shortcut_sampling:
    type: "timestep_first"
    time_sampler: "uniform"
    shortcut_sampler: "uniform"

# Optimizer configuration
optimizer:
  scheduler:
    lr: 1e-3
    type: "linear"
    start_factor: 1.0
    end_factor: 0.01
    total_steps: 5000

#ema: null

# WandB configuration
wandb:
#  project_name: "test_v2"
#  project_name: "shortcutFM"
  project_name: "test"
#  run_name: "lr=1e-4_gc=3_scut=0"
  enabled: true

# Checkpoint configuration
checkpoint:
  save_folder: "checkpoints/qqp"
#  path: "checkpoints/qqp/bert-base/run_hv5ivw2s/last.ckpt"