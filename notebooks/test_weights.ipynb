{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12361d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bc53b6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tied lm_head.weight to word_embedding.weight after pretrained loading\n",
      "word emebedding requires grad: False\n",
      "lm head requires grad: False\n",
      "lm_head tied to word_embedding: True\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "from shortcutfm.__main__ import parse_config\n",
    "from shortcutfm.train.pl.trainer import create_dataloaders\n",
    "from shortcutfm.train.pl.trainer_factory import create_criterion\n",
    "\n",
    "\n",
    "\n",
    "config_path = \"configs/training/qqp.yaml\"\n",
    "cfg = parse_config(config_path, [])\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model.config_name)\n",
    "criterion = create_criterion(cfg, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f8a8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 12:36:54,878 - INFO - Loading dataset...\n",
      "2025-06-22 12:36:54,962 - INFO - Train dataset contains 144715 samples.\n",
      "2025-06-22 12:36:54,971 - INFO - Validation dataset contains 2048 samples.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 2054, 2515,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2024,  ...,    0,    0,    0],\n",
      "        ...,\n",
      "        [ 101, 2054, 2003,  ...,    0,    0,    0],\n",
      "        [ 101, 2190, 2126,  ...,    0,    0,    0],\n",
      "        [ 101, 2054, 2003,  ...,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "# Re-create dataloaders with num_workers=0 for Jupyter compatibility\n",
    "train_dataloader, val_dataloader = create_dataloaders(cfg, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2af9c1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80266f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128])\n"
     ]
    }
   ],
   "source": [
    "print(batch.seqs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c04d047d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] would you rather vote for donald trump or hillary clinton? why? [SEP] [CLS] donald trump or hillary clinton? why? [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "sentences = tokenizer.batch_decode(batch.seqs, skip_special_tokens=False)\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b123d6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 768])\n"
     ]
    }
   ],
   "source": [
    "emebddings = criterion.flow_matching_criterion.model.get_embeddings(batch.seqs)\n",
    "print(emebddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "844754b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 128, 30522])\n"
     ]
    }
   ],
   "source": [
    "predicted_tokens = criterion.flow_matching_criterion.model.compute_logits(emebddings)\n",
    "print(predicted_tokens.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11545370",
   "metadata": {},
   "source": [
    "# Ce loss netween token ids and predicted tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4260c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.334590911865234\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "ce = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "loss = ce(predicted_tokens.view(-1, predicted_tokens.size(-1)), batch.seqs.view(-1))\n",
    "print(loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2e062f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([30522, 768])\n",
      "torch.Size([30522, 768])\n",
      "True\n",
      "9.334590911865234\n"
     ]
    }
   ],
   "source": [
    "# are word_emebddings and and lm_head the same?\n",
    "import torch\n",
    "\n",
    "\n",
    "word_embeddings = criterion.flow_matching_criterion.model.module.word_embedding\n",
    "lm_head = criterion.flow_matching_criterion.model.module.lm_head\n",
    "\n",
    "print(word_embeddings.weight is lm_head.weight)\n",
    "\n",
    "# shapes\n",
    "print(word_embeddings.weight.shape)\n",
    "print(lm_head.weight.shape)\n",
    "# check if weight tensors are identical \n",
    "print(torch.equal(word_embeddings.weight, lm_head.weight))\n",
    "\n",
    "# tensors are the same\n",
    "#weryfiy ce loss \n",
    "predicted_tokens = criterion.flow_matching_criterion.model.compute_logits(emebddings)\n",
    "loss = ce(predicted_tokens.view(-1, predicted_tokens.size(-1)), batch.seqs.view(-1))\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58c6fac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding norm: 1.1510801315307617\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding norm: {emebddings.norm(dim=-1).mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "526e8430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with normalized embeddings: 9.516925811767578\n"
     ]
    }
   ],
   "source": [
    "embeddings_normalized = emebddings / (emebddings.norm(dim=-1, keepdim=True) + 1e-6)\n",
    "logits = criterion.flow_matching_criterion.model.compute_logits(embeddings_normalized)\n",
    "loss = ce(logits.view(-1, logits.size(-1)), batch.seqs.view(-1))\n",
    "print(f\"Loss with normalized embeddings: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce18dff6",
   "metadata": {},
   "source": [
    "# FUcniton to test weights tying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "33889e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_weight_tying(word_embedding, lm_head, vocab_size=10, test_tokens=None):\n",
    "    \"\"\"Test if weight tying is working correctly.\"\"\"\n",
    "\n",
    "    print(\"=== Weight Tying Test ===\")\n",
    "\n",
    "    # Check if weights are actually tied\n",
    "    print(f\"Weights are tied: {lm_head.weight is word_embedding.weight}\")\n",
    "    print(f\"Weight shapes - Embedding: {word_embedding.weight.shape}, LM Head: {lm_head.weight.shape}\")\n",
    "\n",
    "    # Test with a few token IDs\n",
    "    if test_tokens is None:\n",
    "        test_tokens = torch.tensor([0, 1, 2, 100, 500])  # Adjust based on your vocab size\n",
    "\n",
    "    print(f\"\\nTesting with tokens: {test_tokens.tolist()}\")\n",
    "\n",
    "    # Forward pass: tokens -> embeddings -> logits\n",
    "    embeddings = word_embedding(test_tokens)  # [num_tokens, embed_dim]\n",
    "    logits = lm_head(embeddings)  # [num_tokens, vocab_size]\n",
    "\n",
    "    # Get predicted tokens (highest logit)\n",
    "    predicted_tokens = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    print(f\"Original tokens:   {test_tokens.tolist()}\")\n",
    "    print(f\"Predicted tokens:  {predicted_tokens.tolist()}\")\n",
    "    print(f\"Match rate: {(test_tokens == predicted_tokens).float().mean().item():.2%}\")\n",
    "\n",
    "    # Check logit values for original tokens\n",
    "    print(\"\\nLogit analysis:\")\n",
    "    for i, token_id in enumerate(test_tokens):\n",
    "        original_logit = logits[i, token_id].item()\n",
    "        max_logit = logits[i].max().item()\n",
    "        max_token = logits[i].argmax().item()\n",
    "        print(f\"Token {token_id}: logit={original_logit:.3f}, max_logit={max_logit:.3f} (token {max_token})\")\n",
    "\n",
    "    # Test with bias removed (if bias exists)\n",
    "    if lm_head.bias is not None:\n",
    "        print(\"\\n=== Test without bias ===\")\n",
    "        logits_no_bias = lm_head(embeddings) - lm_head.bias\n",
    "        predicted_no_bias = torch.argmax(logits_no_bias, dim=-1)\n",
    "        print(f\"Predicted (no bias): {predicted_no_bias.tolist()}\")\n",
    "        print(f\"Match rate (no bias): {(test_tokens == predicted_no_bias).float().mean().item():.2%}\")\n",
    "\n",
    "    #compute ce loss\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    loss = criterion(logits, test_tokens)\n",
    "    print(f\"\\nCross-Entropy Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ee9a8b",
   "metadata": {},
   "source": [
    "# Tied pretrained embedding weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e046c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 9.256082534790039\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "\n",
    "# Initialize tokenizer and BERT model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create embedding layer and initialize with pretrained weights\n",
    "embedding = nn.Embedding(30522, 768)\n",
    "with torch.no_grad():\n",
    "    embedding.weight.copy_(bert.embeddings.word_embeddings.weight)\n",
    "\n",
    "# Create lm_head and tie weights to embedding\n",
    "lm_head = nn.Linear(768, 30522, bias=True)\n",
    "with torch.no_grad():\n",
    "    lm_head.weight = embedding.weight  # Tie weights\n",
    "    lm_head.bias.zero_()  # Initialize bias to zero\n",
    "\n",
    "# Freeze weights\n",
    "embedding.weight.requires_grad = False\n",
    "lm_head.weight.requires_grad = False\n",
    "\n",
    "# Prepare input\n",
    "text = [\"This is a test sentence.\"]\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# Compute embeddings and logits\n",
    "embeddings = embedding(input_ids)\n",
    "logits = lm_head(embeddings)\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "ce = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "loss = ce(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "print(f\"Cross-entropy loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039d9def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Weight Tying Test ===\n",
      "Weights are tied: True\n",
      "Weight shapes - Embedding: torch.Size([30522, 768]), LM Head: torch.Size([30522, 768])\n",
      "\n",
      "Testing with tokens: [101, 2023, 2003, 1037, 3231, 6251, 1012, 102]\n",
      "Original tokens:   [101, 2023, 2003, 1037, 3231, 6251, 1012, 102]\n",
      "Predicted tokens:  [101, 2023, 2003, 1037, 3231, 6251, 1012, 101]\n",
      "Match rate: 87.50%\n",
      "\n",
      "Logit analysis:\n",
      "Token 101: logit=4.124, max_logit=4.124 (token 101)\n",
      "Token 2023: logit=0.816, max_logit=0.816 (token 2023)\n",
      "Token 2003: logit=0.760, max_logit=0.760 (token 2003)\n",
      "Token 1037: logit=0.758, max_logit=0.758 (token 1037)\n",
      "Token 3231: logit=1.310, max_logit=1.310 (token 3231)\n",
      "Token 6251: logit=1.555, max_logit=1.555 (token 6251)\n",
      "Token 1012: logit=0.623, max_logit=0.623 (token 1012)\n",
      "Token 102: logit=0.586, max_logit=0.643 (token 101)\n",
      "\n",
      "=== Test without bias ===\n",
      "Predicted (no bias): [101, 2023, 2003, 1037, 3231, 6251, 1012, 101]\n",
      "Match rate (no bias): 87.50%\n",
      "\n",
      "Cross-Entropy Loss: 9.2561\n"
     ]
    }
   ],
   "source": [
    "test_weight_tying(embedding, lm_head, vocab_size=30522, test_tokens=input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6fd8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-entropy loss: 10.014132499694824\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Create embedding layer with random weights\n",
    "embedding = nn.Embedding(30522, 768)\n",
    "nn.init.normal_(embedding.weight, mean=0.0, std=0.02)  # Random initialization with small std\n",
    "# embedding.weight.requires_grad = False\n",
    "\n",
    "# Create lm_head and tie weights to embedding\n",
    "lm_head = nn.Linear(768, 30522, bias=False)\n",
    "with torch.no_grad():\n",
    "    lm_head.weight = embedding.weight  # Tie weights\n",
    "    # lm_head.bias.zero_()  # Initialize bias to zero\n",
    "# lm_head.weight.requires_grad = False\n",
    "\n",
    "\n",
    "# Prepare input\n",
    "text = [\"This is a test sentence.\"]\n",
    "encoding = tokenizer(text, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = encoding[\"input_ids\"]\n",
    "\n",
    "# Compute embeddings and logits\n",
    "embeddings = embedding(input_ids)\n",
    "logits = lm_head(embeddings)\n",
    "\n",
    "# Compute cross-entropy loss\n",
    "ce = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n",
    "loss = ce(logits.view(-1, logits.size(-1)), input_ids.view(-1))\n",
    "print(f\"Cross-entropy loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011fd01e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e50d89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_weight_tying(embedding, lm_head, vocab_size=30522, test_tokens=input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f4e94463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding norm: 1.077894687652588\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding norm: {embeddings.norm(dim=-1).mean().item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
